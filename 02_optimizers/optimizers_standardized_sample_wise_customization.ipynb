{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MaksutovRN\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import CSVLogger\n",
    "import os\n",
    "import keras.activations\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "def standardize_sample_wise(dataset):\n",
    "    standardized = []\n",
    "    for s in dataset:\n",
    "        mean, std = s.mean(), s.std()\n",
    "        s = (s - mean) / std\n",
    "        standardized.append(s)\n",
    "    return np.array(standardized)\n",
    "\n",
    "x_train = standardize_sample_wise(x_train)\n",
    "x_test = standardize_sample_wise(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "units = 64\n",
    "experiments = 5\n",
    "start = 0\n",
    "# activations = ['sigmoid', 'tanh', 'relu', 'elu', 'selu', 'softplus', 'softsign', 'hard_sigmoid', 'LeakyReLU', 'ThresholdedReLU']\n",
    "activations = ['ThresholdedReLU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.1, 'decay': 0.0, 'nesterov': False },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.9, 'decay': 0.0, 'nesterov': False },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.95, 'decay': 0.0, 'nesterov': False },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.99, 'decay': 0.0, 'nesterov': False },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.999, 'decay': 0.0, 'nesterov': False },\n",
    "\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.0, 'decay': 0.0, 'nesterov': True },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.1, 'decay': 0.0, 'nesterov': True },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.9, 'decay': 0.0, 'nesterov': True },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.95, 'decay': 0.0, 'nesterov': True },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.99, 'decay': 0.0, 'nesterov': True },\n",
    "#     { 'optimizer': 'sgd', 'momentum': 0.999, 'decay': 0.0, 'nesterov': True },\n",
    "    \n",
    "#     { 'optimizer': 'rmsp', 'rho': 0.5, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'rmsp', 'rho': 0.99, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'rmsp', 'rho': 0.95, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'rmsp', 'rho': 0.999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'rmsp', 'rho': 0.9999, 'decay': 0.0 },\n",
    "\n",
    "#     { 'optimizer': 'Adadelta', 'rho': 0.5, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'Adadelta', 'rho': 0.9, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'Adadelta', 'rho': 0.99, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'Adadelta', 'rho': 0.999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'Adadelta', 'rho': 0.9999, 'decay': 0.0 },\n",
    "    \n",
    "#     { 'optimizer': 'adam', 'amsgrad': False, 'beta_1': 0.95, 'beta_2': 0.9999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'adam', 'amsgrad': False, 'beta_1': 0.9, 'beta_2': 0.999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'adam', 'amsgrad': False, 'beta_1': 0.95, 'beta_2': 0.999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'adam', 'amsgrad': False, 'beta_1': 0.9, 'beta_2': 0.9999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'adam', 'amsgrad': True, 'beta_1': 0.95, 'beta_2': 0.9999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'adam', 'amsgrad': True, 'beta_1': 0.9, 'beta_2': 0.999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'adam', 'amsgrad': True, 'beta_1': 0.95, 'beta_2': 0.999, 'decay': 0.0 },\n",
    "#     { 'optimizer': 'adam', 'amsgrad': True, 'beta_1': 0.9, 'beta_2': 0.9999, 'decay': 0.0 },\n",
    "    \n",
    "#     { 'optimizer': 'Adamax', 'beta_1': 0.99, 'beta_2': 0.9999, 'decay': 0.0 },    \n",
    "#     { 'optimizer': 'Adamax', 'beta_1': 0.99, 'beta_2': 0.999, 'decay': 0.0 },    \n",
    "#     { 'optimizer': 'Adamax', 'beta_1': 0.95, 'beta_2': 0.999, 'decay': 0.0 },      \n",
    "#     { 'optimizer': 'Adamax', 'beta_1': 0.95, 'beta_2': 0.99, 'decay': 0.0 },      \n",
    "#     { 'optimizer': 'Adamax', 'beta_1': 0.95, 'beta_2': 0.95, 'decay': 0.0 },      \n",
    "#     { 'optimizer': 'Adamax', 'beta_1': 0.9, 'beta_2': 0.9, 'decay': 0.0 },    \n",
    "#     { 'optimizer': 'Adamax', 'beta_1': 0.9, 'beta_2': 0.99, 'decay': 0.0 },    \n",
    "#     { 'optimizer': 'Adamax', 'beta_1': 0.9, 'beta_2': 0.95, 'decay': 0.0 },        \n",
    "    \n",
    "#     { 'optimizer': 'Nadam', 'beta_1': 0.9, 'beta_2': 0.99, 'schedule_decay': 0.004 },        \n",
    "#     { 'optimizer': 'Nadam', 'beta_1': 0.95, 'beta_2': 0.999, 'schedule_decay': 0.004 },        \n",
    "#     { 'optimizer': 'Nadam', 'beta_1': 0.95, 'beta_2': 0.99, 'schedule_decay': 0.004 },        \n",
    "#     { 'optimizer': 'Nadam', 'beta_1': 0.95, 'beta_2': 0.95, 'schedule_decay': 0.004 },        \n",
    "#     { 'optimizer': 'Nadam', 'beta_1': 0.95, 'beta_2': 0.9, 'schedule_decay': 0.004 },        \n",
    "#     { 'optimizer': 'Nadam', 'beta_1': 0.9, 'beta_2': 0.9, 'schedule_decay': 0.004 },        \n",
    "    { 'optimizer': 'Nadam', 'beta_1': 0.9, 'beta_2': 0.95, 'schedule_decay': 0.004 },        \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for activation ThresholdedReLU with config {'optimizer': 'Nadam', 'beta_1': 0.9, 'beta_2': 0.95, 'schedule_decay': 0.004}, experiment 0\n",
      "Remaining time: 0 days 0 hours 15 minutes 40 seconds\n",
      "Training for activation ThresholdedReLU with config {'optimizer': 'Nadam', 'beta_1': 0.9, 'beta_2': 0.95, 'schedule_decay': 0.004}, experiment 1\n",
      "Remaining time: 0 days 0 hours 11 minutes 35 seconds\n",
      "Training for activation ThresholdedReLU with config {'optimizer': 'Nadam', 'beta_1': 0.9, 'beta_2': 0.95, 'schedule_decay': 0.004}, experiment 2\n",
      "Remaining time: 0 days 0 hours 07 minutes 42 seconds\n",
      "Training for activation ThresholdedReLU with config {'optimizer': 'Nadam', 'beta_1': 0.9, 'beta_2': 0.95, 'schedule_decay': 0.004}, experiment 3\n",
      "Remaining time: 0 days 0 hours 03 minutes 50 seconds\n",
      "Training for activation ThresholdedReLU with config {'optimizer': 'Nadam', 'beta_1': 0.9, 'beta_2': 0.95, 'schedule_decay': 0.004}, experiment 4\n",
      "Remaining time: 0 days 0 hours 00 minutes 00 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "counter = 0\n",
    "total_items = len(activations) * experiments * len(configs)\n",
    "\n",
    "for cfg in configs:\n",
    "\n",
    "    for act in activations:\n",
    "\n",
    "        for i in range(experiments):\n",
    "            \n",
    "            print(\"Training for activation %s with config %s, experiment %d\" % (act, str(cfg), i))\n",
    "            \n",
    "            K.clear_session()\n",
    "            K.reset_uids()\n",
    "            \n",
    "            act_dict = {\n",
    "              'sigmoid': Activation(keras.activations.sigmoid),\n",
    "              'tanh': Activation(keras.activations.tanh),\n",
    "              'relu': Activation(keras.activations.relu),\n",
    "              'linear': Activation(keras.activations.linear),\n",
    "              'elu': Activation(keras.activations.elu),\n",
    "              'softplus': Activation(keras.activations.softplus),\n",
    "              'softsign': Activation(keras.activations.softsign),\n",
    "              'hard_sigmoid': Activation(keras.activations.hard_sigmoid),\n",
    "              'LeakyReLU': keras.layers.advanced_activations.LeakyReLU(),\n",
    "              'selu': Activation(keras.activations.selu),\n",
    "              'ThresholdedReLU': keras.layers.advanced_activations.ThresholdedReLU(theta=0.7) \n",
    "            }\n",
    "\n",
    "            if cfg['optimizer'] == 'sgd':\n",
    "                optimizer = keras.optimizers.SGD(momentum=cfg['momentum'], decay=cfg['decay'], nesterov=cfg['nesterov'])\n",
    "                model_name = '_'.join(['std_sample_wise', act, cfg['optimizer'], \n",
    "                                       str(cfg['momentum']), str(cfg['decay']), \n",
    "                                       str(cfg['nesterov']), str(i + start), str(units)])\n",
    "            \n",
    "            elif cfg['optimizer'] == 'rmsp':\n",
    "                optimizer = keras.optimizers.rmsprop(lr=0.001, rho=cfg['rho'], decay=cfg['decay'])\n",
    "                model_name = '_'.join(['std_sample_wise', act, cfg['optimizer'], \n",
    "                                       str(cfg['rho']), str(cfg['decay']), \n",
    "                                       str(i + start), str(units)])\n",
    "            \n",
    "            elif cfg['optimizer'] == 'Adadelta':\n",
    "                optimizer = keras.optimizers.Adadelta(rho=cfg['rho'], decay=cfg['decay'])\n",
    "                model_name = '_'.join(['std_sample_wise', act, cfg['optimizer'], \n",
    "                                       str(cfg['rho']), str(cfg['decay']), \n",
    "                                       str(i + start), str(units)])\n",
    "            \n",
    "            elif cfg['optimizer'] == 'adam':\n",
    "                optimizer = keras.optimizers.Adam(amsgrad=cfg['amsgrad'], decay=cfg['decay'], \n",
    "                                                  beta_1=cfg['beta_1'], beta_2=cfg['beta_2'])\n",
    "                model_name = '_'.join(['std_sample_wise', act, cfg['optimizer'], \n",
    "                                       str(cfg['amsgrad']), str(cfg['beta_1']), str(cfg['beta_2']), \n",
    "                                       str(cfg['decay']), str(i + start), str(units)])\n",
    "                \n",
    "            elif cfg['optimizer'] == 'Adagrad':\n",
    "                optimizer = keras.optimizers.Adagrad(decay=cfg['decay'])\n",
    "                model_name = '_'.join(['std_sample_wise', act, cfg['optimizer'], \n",
    "                                       str(cfg['decay']), str(i + start), str(units)])\n",
    "                \n",
    "            elif cfg['optimizer'] == 'Adamax':\n",
    "                optimizer = keras.optimizers.Adamax(decay=cfg['decay'], \n",
    "                                                  beta_1=cfg['beta_1'], beta_2=cfg['beta_2'])\n",
    "                model_name = '_'.join(['std_sample_wise', act, cfg['optimizer'], str(cfg['beta_1']), str(cfg['beta_2']),\n",
    "                                       str(cfg['decay']), str(i + start), str(units)])\n",
    "                \n",
    "            elif cfg['optimizer'] == 'Nadam':\n",
    "                optimizer = keras.optimizers.Nadam(schedule_decay=cfg['schedule_decay'], \n",
    "                                                  beta_1=cfg['beta_1'], beta_2=cfg['beta_2'])\n",
    "                model_name = '_'.join(['std_sample_wise', act, cfg['optimizer'], str(cfg['beta_1']), str(cfg['beta_2']), \n",
    "                                       str(cfg['schedule_decay']), str(i + start), str(units)])\n",
    "                \n",
    "                \n",
    "            inputs = Input(shape=(784,))\n",
    "            x = Dense(units)(inputs)\n",
    "            x = act_dict[act](x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(units)(x)\n",
    "            x = act_dict[act](x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            predictions = Dense(num_classes, activation='softmax')(x)\n",
    "            model = Model(inputs=inputs, outputs=predictions)\n",
    "                \n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=optimizer,\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "            csv_logger = CSVLogger('./training_logs/custom/%s.csv' % (model_name), append=False)\n",
    "            history = model.fit(x_train, y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs,\n",
    "                                verbose=0,\n",
    "                                validation_data=(x_test, y_test), callbacks=[csv_logger])\n",
    "\n",
    "#             score = model.evaluate(x_test, y_test, verbose=0)\n",
    "#             print('Test loss:', score[0])\n",
    "#             print('Test accuracy:', score[1])\n",
    "            \n",
    "            t = time.time()\n",
    "            time_diff = t - start_time\n",
    "            counter +=1\n",
    "            rem_items = total_items - counter\n",
    "            total_time = round((total_items / counter) * time_diff)\n",
    "            rem_time = round(total_time - time_diff)\n",
    "            m, s = divmod(rem_time, 60)\n",
    "            h, m = divmod(m, 60)\n",
    "            d, h = divmod(h, 24)\n",
    "            print('Remaining time: %d days %d hours %02d minutes %02d seconds' % (d, h, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
